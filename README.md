# Apache Airflow Learning Repo ğŸš€

Welcome to my Airflow learning repository!  
This is a hands-on space where I'm learning how to build and orchestrate data pipelines using **Apache Airflow 3.0.2**, with **Docker**, **VS Code**, and **Python**.

## ğŸ› ï¸ Tech Stack

- **Airflow**: v3.0.2  
- **Docker**: For containerized development  
- **Python**: DAGs, sensors, assets, and task logic  
- **VS Code**: My primary development environment  
- **PostgreSQL**: Metadata store and database  
- **Celery**: For distributed task execution  
- **LocalExecutor / CeleryExecutor**: For testing queueing and parallel execution  

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ user_processing.py   # My first DAG
â”‚   â””â”€â”€ user.py              # Asset-based data processing example
â”œâ”€â”€ docker-compose.yml       # Docker environment for Airflow + Postgres
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ airflow.cfg              # Optional: custom Airflow config
â””â”€â”€ README.md                # You are here :)
```

## ğŸ“„ DAGs in This Repo

### âœ… `user_processing.py`
- My first DAG built using `@dag` and `@task` decorators.
- Includes:
  - Creating a `users` table in PostgreSQL
  - Sensor to check API availability
  - Extract â†’ Process â†’ Load pipeline using temporary CSV
  - Demonstrates task chaining and dependency management

### âœ… `user.py`
- Demonstrates Airflow **assets**
- Uses `@asset(schedule=...)` for declarative scheduling
- Simulates extracting structured data from an API (randomuser)

## âš™ï¸ Features Iâ€™m Exploring

- [x] Writing DAGs using Python  
- [x] Using `@task` and `@task.sensor` decorators  
- [x] Using `@asset` from the new asset-based API  
- [x] Using **Docker Compose** to run Airflow locally  
- [x] Managing workflows via the **Airflow UI**  
- [x] Parallel task execution with **CeleryExecutor**  
- [x] Understanding **task queues**  
- [x] Implementing **Task Groups** for DAG organization  
- [ ] Creating dynamic DAGs  
- [ ] Connecting with GCP (BigQuery, GCS)  

## ğŸ³ Running Locally with Docker

1. Clone the repo:
    ```bash
    git clone https://github.com/cslcalderon/airflow-learning.git
    cd airflow-learning
    ```

2. Start the Airflow environment:
    ```bash
    docker compose up
    ```

3. Access the Airflow UI at:  
   [http://localhost:8080](http://localhost:8080)

   Default credentials:  
   - **Username**: `airflow`  
   - **Password**: `airflow`

4. To trigger DAGs, either:  
   - Use the Airflow UI  
   - Or run from CLI:
     ```bash
     docker exec -it <webserver_container_name> airflow dags trigger user_processing
     ```

## ğŸ’¬ Why Iâ€™m Doing This

Data engineering is where software meets data at scale.  
As someone with a strong foundation in software engineering and data science, Airflow feels like the perfect bridge to automate, orchestrate, and scale meaningful data workflows.

This repo is my playground to:
- Learn orchestration best practices  
- Understand distributed task execution  
- Optimize end-to-end ETL/ELT pipelines  

## ğŸš€ Whatâ€™s Next

This project confirmed something important for me:

> **Data engineering is exactly where I want to go.**

Itâ€™s the intersection of everything I care about: automation, optimization, coding, and working with meaningful data.

**Next steps:**

- âœ… Build a more advanced DAG with branching logic  
- â˜ï¸ Integrate with Google Cloud (BigQuery + Cloud Storage)  
- ğŸ” Explore `dbt` and `Kafka` for batch and stream processing  
- ğŸ³ Keep refining my Docker and deployment skills  

## ğŸ“š Resources

- ğŸ“ [Udemy: The Complete Hands-On Course to Master Apache Airflow](https://www.udemy.com/course/the-complete-hands-on-course-to-master-apache-airflow/)  
- ğŸ“– [Airflow Docs (v3.0+)](https://airflow.apache.org/docs/apache-airflow/stable/index.html)

## ğŸ™‹â€â™€ï¸ About Me

I'm Sofia Calderon, a data-driven learner passionate about using tech for impact.  
Check out my [GitHub](https://github.com/your-username) or connect with me on [LinkedIn](https://www.linkedin.com/in/your-linkedin/)
